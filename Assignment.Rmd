---
title: "Prediction Assignment"
output: html_document
---
**Load Packages**
```{r}
library(caret)
library(rattle)
library(RCurl)
```

**Get Data, prepare for model building**
We are looking at a table with 151 potential predictor variables. 
```{r}
data <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
               ssl.verifypeer=0L, followlocation=1L)
training <- read.csv(text=data, na.strings=c("NA","#DIV/0!",""))

data <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
               ssl.verifypeer=0L, followlocation=1L)
testing <- read.csv(text=data, na.strings=c("NA","#DIV/0!",""))
#Drop columns that don't add value for our predicion
training2 <- training[, -seq(from = 1, to = 8, by = 1)]
set.seed(244)

# Create additional Test Subset withing 40% of Total Training Data
# Training Subset with 60% of Total Training Data
inTest <- createDataPartition(y = training2$classe, p = 0.4, list = F)
test <- training2[inTest, ]
train <- training2[-inTest, ]
```

This dataset has a lot of variables that are sparse. To make our predictiors  more relevant it might be a good idea to get rid of variables that are sparse. Lets drop all that are NA for more than 15% of the observations.

```{r}
removeNAcols   <- function(x) { 
        x[ , colSums( is.na(x) ) < nrow(x) ] 
}
train <- removeNAcols(train)
test  <- removeNAcols(test)

removeanyNA       <- function(x) {
        x[,sapply(x, function(y) !any(is.na(y)))] 
}

train <- removeanyNA(train)
test  <- removeanyNA(test)

```

**Prediction Model** 
I will go ahead and use random forest to predict 'classe'. This prediction method is especially well suited for large number of variables or predictors.

```{r}

random.forest <- train(train[,-52],
                       train$classe,
                       tuneGrid=data.frame(mtry=3),
                       trControl=trainControl(method="none")
                       )
```

**Results**
Here are the results of the prediction model building
```{r, echo=FALSE }
summary(random.forest)
plot(varImp(random.forest))
```

** Estimate Out of sample error with Cross Validation**
Our final decision tree was build using the most accurate ones using the train data set. To estimate the real out of sample error we have to do cross validation with a still untouched data set. Good thing we left 40% of the original test data still untouched. 

Generating the confusion matrix and statistics shows that our random forest  works well to predict accelerometer measurements. The accuracy can be estimated to be is .991.
```{r}
confusionMatrix(predict(random.forest,
                        newdata=test[,-52]),
                test$classe
                )
```


